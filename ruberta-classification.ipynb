{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4893684,"sourceId":8248099,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's store the csv dataset into a DataFrame and have a look at it\n# Ссылка на скачивание данных в формате CSV\ncsv_data_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vT3Wns_gLiMDfacVgF-x4suNvnAFmgibVkrsizkKqwcVFGSYAVOVnJsoV7gm3jguw/pub?gid=681978523&single=true&output=csv\"\n\n# Загрузка данных в DataFrame\ndata = pd.read_csv(csv_data_url)\n\n# Просмотр первых нескольких строк данных\nprint(data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets count duplicated entries in the problem_text column\n# 'problem_text' имя столбца, в котором нужно найти дубликаты\nduplicates = data.duplicated(subset=['problem_text'], keep=False)\n\n# Считаем количество дубликатов\nnum_duplicates = duplicates.sum()\n\n# Выводим количество дубликатов\nprint(f'Количество дубликатов в столбце \"problem_text\": {num_duplicates}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll leave the first entries and we'll drop the duplicates \n# Удаляем дубликаты, оставляя только первые вхождения\ndata_no_dupes = data.drop_duplicates(subset=['problem_text'], keep='first')\n\n# Перезаписываем индексы после удаления дубликатов\ndata_no_dupes.reset_index(drop=True, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to check the results. We expect 0 duplicated entries in the column we cleaned\n#смотрим, как удалили \nduplicates = data_no_dupes.duplicated(subset=['problem_text'], keep=False)\nnum_duplicates = duplicates.sum()\nprint(f'Количество дубликатов в столбце \"problem_text\": {num_duplicates}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A quick visual aid to check the distribution over classes\n# распределение по классам\nimport matplotlib.pyplot as plt\n\nclass_counts = data_no_dupes['topic'].value_counts()\nclass_counts.plot(kind='bar')\nplt.title('New Distribution of Classes')\nplt.xlabel('Topics')\nplt.xticks(rotation=45)\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll need to augment the datset to balance the distribution \n# but beforehand we should split the data set into test train val sets \n#разобьем на test train val\nfrom sklearn.model_selection import train_test_split\n\n# Разделение датасета\ntrain, test_val = train_test_split(data_no_dupes, test_size=0.4, random_state=42)\nval, test = train_test_split(test_val, test_size=0.5, random_state=42)\n\n# сохраним\ntrain.to_csv('train.csv', index=False)\nval.to_csv('val.csv', index=False)\ntest.to_csv('test.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll augment the train set with https://github.com/makcedward/nlpaug \n# будем выравнивать датасет по классам с помощью https://github.com/makcedward/nlpaug \n!pip install nlpaug numpy matplotlib python-dotenv\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see how the classes are represeted in our train set\nimport matplotlib.pyplot as plt\n\nclass_counts = train['topic'].value_counts()\nclass_counts.plot(kind='bar')\nplt.title('New Distribution of Classes')\nplt.xlabel('Topics')\nplt.xticks(rotation=45)\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.autograd.set_detect_anomaly(True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for Kaggle & Bert tokenizer\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall tensorflow -y\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# actual augmenting happens here\n\nimport nlpaug.augmenter.word as naw\nimport pandas as pd\nimport torch\ntorch.set_printoptions(profile=\"full\")\n\ntorch.set_num_threads(1) # I had to do that due to some obscure mistake\n\n\n# Инициализация аугментатора BERT\n# aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cpu')\n# aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cuda')\n# Use PyTorch as the backend\naug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", device='cuda', force_reload=True)\n\n# Определение максимального количества примеров в классе\nmax_samples = train['topic'].value_counts().max()\n\n# Создание пустого DataFrame для аугментированных данных\naugmented_data = pd.DataFrame(columns=train.columns)\n\n# Аугментация данных\nfor topic in train['topic'].unique():\n    # Вычисление необходимого количества аугментаций для класса\n    current_count = train[train['topic'] == topic].shape[0]\n    augment_count = max_samples - current_count\n    \n    if augment_count > 0:\n        # Выборка случайных примеров для аугментации\n        sample = train[train['topic'] == topic].sample(n=augment_count, replace=True)\n        \n        # Применение аугментации\n        sample['problem_text'] = sample['problem_text'].apply(lambda x: aug.augment(x))\n        \n        # Добавление аугментированных примеров в датасет\n        augmented_data = pd.concat([augmented_data, sample])\n\n# Объединение аугментированных данных с исходным обучающим датасетом\ntrain_augmented = pd.concat([train, augmented_data])\n\n# Перемешиваем данные\ntrain_augmented = train_augmented.sample(frac=1).reset_index(drop=True)\n\n# Сохраняем аугментированный датасет в новый CSV-файл\ntrain_augmented.to_csv('train_augmented.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_augmented)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets marvel at the augmeted train set\nimport matplotlib.pyplot as plt\n\nclass_counts = train_augmented['topic'].value_counts()\nclass_counts.plot(kind='bar')\nplt.title('New Distribution of Classes')\nplt.xlabel('Topics')\nplt.xticks(rotation=45)\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There could be some duplicates creeping in after the augmetation\nduplicates = train_augmented.duplicated(subset=['problem_text'], keep=False)\n\nnum_duplicates = duplicates.sum()\n\nprint(f'Количество дубликатов в столбце \"problem_text\": {num_duplicates}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading neccessities\n\nimport os\nfrom IPython.display import FileLink\n\n# Путь к файлу на вашем ноутбуке Kaggle\nfile_path = 'train_augmented.csv'\n\n# Проверяем, существует ли файл\nif os.path.isfile(file_path):\n    # Создаем ссылку для скачивания\n    download_link = FileLink(file_path, result_html_prefix=\"Click here to download: \")\n    display(download_link)\nelse:\n    print(\"Файл не найден\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Путь к файлу на вашем ноутбуке Kaggle\nfile_path = 'val.csv'\n\n# Проверяем, существует ли файл\nif os.path.isfile(file_path):\n    # Создаем ссылку для скачивания\n    download_link = FileLink(file_path, result_html_prefix=\"Click here to download: \")\n    display(download_link)\nelse:\n    print(\"Файл не найден\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Путь к файлу на вашем ноутбуке Kaggle\nfile_path = 'test.csv'\n\n# Проверяем, существует ли файл\nif os.path.isfile(file_path):\n    # Создаем ссылку для скачивания\n    download_link = FileLink(file_path, result_html_prefix=\"Click here to download: \")\n    display(download_link)\nelse:\n    print(\"Файл не найден\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_augmented['problem_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\n\n# Assume train_augmented, val, and test are DataFrames\n# Convert the column \"problem_text\" to string format\ntrain_augmented['problem_text'] = train_augmented['problem_text'].astype(str)\nval['problem_text'] = val['problem_text'].astype(str)\ntest['problem_text'] = test['problem_text'].astype(str)\n\n# Convert the pandas DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_augmented)\nval_dataset = Dataset.from_pandas(val)\ntest_dataset = Dataset.from_pandas(test)\n\n# Load the tokenizer\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n\n# Tokenize function\ndef tokenize_function(examples):\n    return tokenizer(examples['problem_text'], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Ensure labels are included\ntrain_dataset = train_dataset.map(lambda examples: {'labels': examples['topic']}, batched=True)\nval_dataset = val_dataset.map(lambda examples: {'labels': examples['topic']}, batched=True)\n\n# Set the format for PyTorch\ntrain_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nval_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AdamW\nimport torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers.trainer_utils import EvalPrediction\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode labels into integers\nlabel_encoder = LabelEncoder()\ntrain_augmented['labels'] = label_encoder.fit_transform(train_augmented['topic'])\nval['labels'] = label_encoder.transform(val['topic'])\ntest['labels'] = label_encoder.transform(test['topic'])\n\n# Convert the DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_augmented)\nval_dataset = Dataset.from_pandas(val)\ntest_dataset = Dataset.from_pandas(test)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n\n# Tokenize function\ndef tokenize_function(examples):\n    return tokenizer(examples['problem_text'], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set the format for PyTorch\ntrain_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nval_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Check first element\n# print(train_dataset[0])\n\n# Define the number of classes\nnum_labels = len(label_encoder.classes_)\n\n# Load the model\nmodel = AutoModelForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=num_labels)\n\n# Unfreeze all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Define the compute metrics function\ndef compute_metrics(eval_pred: EvalPrediction):\n    logits, labels = eval_pred\n    # Convert logits to a PyTorch tensor\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    accuracy = accuracy_score(labels, predictions)\n    precision = precision_score(labels, predictions, average='macro')\n    recall = recall_score(labels, predictions, average='macro')\n    f1 = f1_score(labels, predictions, average='macro')\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Try different learning rates\nbest_f1 = 0\nbest_lr = 0\nbest_trainer = None\nfor lr in [1e-5]:  #  , 2e-5, 3e-5\n    print(f\"Training with learning rate: {lr}\")\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=5,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=100,\n        save_steps=5000,\n        save_total_limit=2,\n        learning_rate=lr,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        optimizers=(AdamW(model.parameters(), lr=lr), None),\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    evaluation = trainer.evaluate()\n    f1 = evaluation.get('eval_f1', 0)\n\n    if f1 > best_f1:\n        best_f1 = f1\n        best_lr = lr\n        best_trainer = trainer\n\nprint(f\"Best learning rate: {best_lr} with F1 Score: {best_f1}\")\n\n# Use the best trainer for predictions\npredictions = best_trainer.predict(val_dataset)\ntrue_labels = predictions.label_ids\npredicted_labels = torch.argmax(torch.tensor(predictions.predictions), axis=-1)\n\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels, average='macro', zero_division=1)\nrecall = recall_score(true_labels, predicted_labels, average='macro', zero_division=1)\nf1 = f1_score(true_labels, predicted_labels, average='macro')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}